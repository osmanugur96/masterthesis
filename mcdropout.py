# -*- coding: utf-8 -*-
"""MCDropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xRNOYGpgBT6_sUQnlE2GMQznja3yEQQT
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from deepdow.benchmarks import Benchmark, OneOverN, Random
from deepdow.callbacks import EarlyStoppingCallback
from deepdow.data import InRAMDataset, RigidDataLoader, prepare_standard_scaler, Scale
from deepdow.data.synthetic import sin_single
from deepdow.experiments import Run
from deepdow.layers import SoftmaxAllocator, RNN, NumericalMarkowitz, AverageCollapse, AttentionCollapse, CovarianceMatrix
from deepdow.losses import MeanReturns, SharpeRatio, MaximumDrawdown, SortinoRatio, Alpha, CumulativeReturn, RiskParity, WorstReturn
from deepdow.visualize import generate_metrics_table, generate_weights_table, plot_metrics, plot_weight_heatmap, generate_cumrets
from deepdow.nn import BachelierNet
import matplotlib.pyplot as plt
import numpy as np
import torch

from deepdow.utils import prices_to_returns

import seaborn as sns

torch.manual_seed(10)
np.random.seed(54)

"""## Data Creation -Ticker selection"""

import os

# load the key from the enviroment variables
api_key = os.environ.get('API_EOD')

from eod import EodHistoricalData

# Create the instance 
client = EodHistoricalData(api_key)
# predefine some instruments
symbol='BTC'
goverment_bond = 'SW10Y.GBOND'

# Quick usage
# weekly prices for the Swiss goverment bond
stock_prices = client.get_prices_eod(symbol, period='w', order='a')

stock_prices

def enable_dropout(network):
    """Function to enable the dropout layers during test-time"""
    network.dropout_layer.train()

import pandas as pd
import yfinance as yf
from yahoofinancials import YahooFinancials

raw_df = yf.download(
    "LNSTY EEM EXS1.DE XLP GOLD XLF SLV XLK BOIL XLE",
    start="2019-01-02",
    end="2022-09-23",
    progress=True,
)
raw_df.columns = raw_df.columns.swaplevel(0, 1)
raw_df.sort_index(axis=1, level=0, inplace=True)
tickers = "LNSTY EEM DAX VIX GOLD JPM SLV WMT BOIL NDX"

data=raw_df.iloc[:,raw_df.columns.get_level_values(1)=='Close']

raw_df

data

returns = prices_to_returns(data)

returns

plt.figure(figsize=(12, 6))
heatmap = sns.heatmap(returns.corr(), vmin=-1, vmax=1, annot=True)
plt.title('Correlation matrix')
plt.show()

covmat= returns.cov()

covmat

mean=returns.mean()

mean

df_risk_ret = pd.DataFrame({'risk': np.diag(covmat) ** (1 / 2),
                            'return': mean})

x_lim = (0, df_risk_ret['risk'].max() * 1.1)
y_lim = (df_risk_ret['return'].min() * 1.1, df_risk_ret['return'].max() * 1.1)
ax = df_risk_ret.plot.scatter(x='risk', y='return')
ax.set_xlim(x_lim)
ax.set_ylim(y_lim)
plt.tight_layout()

n_timesteps = 1500
returns = pd.DataFrame(np.random.multivariate_normal(mean.values,
                                                     covmat.values,
                                                     size=n_timesteps,
                                                     check_valid='raise'), columns=mean.index)

sns.violinplot(data=returns.iloc[:, ::3])

raw_df
assert isinstance(raw_df, pd.DataFrame)
assert isinstance(raw_df.index, pd.DatetimeIndex)
assert isinstance(raw_df.columns, pd.MultiIndex)

np.expand_dims(raw_df,axis=0)

import deepdow
from deepdow.utils import raw_to_Xy


n_timesteps = len(raw_df) 
n_channels = len(raw_df.columns.levels[1])
n_assets = len(raw_df.columns.levels[0])

lookback, gap, horizon = 21, 1, 1

X, timestamps, y, asset_names, indicators = raw_to_Xy(raw_df,
                                                      lookback=lookback,
                                                      gap=gap,
                                                      freq="B",
                                                      horizon=horizon)

n_samples =  n_timesteps - lookback - horizon - gap + 1  # 10

split_ix = int(n_samples * 0.8)
indices_train = list(range(split_ix))
indices_test = list(range(split_ix + lookback + horizon, n_samples))

print('Train range: {}:{}\nTest range: {}:{}'.format(indices_train[0], indices_train[-1],
                                                     indices_test[0], indices_test[-1]))

means, stds = prepare_standard_scaler(X, indices=indices_train)
print('mean: {}, std: {}'.format(means, stds))

dataset = InRAMDataset(X, y, transform=Scale(means, stds))

dataloader_train = RigidDataLoader(dataset,
                                   indices=indices_train,
                                   batch_size=32)

dataloader_test = RigidDataLoader(dataset,
                                  indices=indices_test,
                                  batch_size=32)

test = iter(dataset)
next(test)
a,b,c,d= next(test)
a.shape, b.shape, c, d

"""## Model Creation"""

loss = SortinoRatio()

n_input_channels = 6
n_assets = 10
max_weight = 0.2
hidden_size = 32
network = BachelierNet(n_input_channels, n_assets, hidden_size=hidden_size, max_weight=max_weight)

print(network)

run = Run(network,
          loss,
          dataloader_train,
          val_dataloaders={'test': dataloader_test},
          optimizer=torch.optim.Adam(network.parameters(), amsgrad=True),
          callbacks=[EarlyStoppingCallback(metric_name='loss',
                                           dataloader_name='test',
                                           patience=15)])

inp1,inp2,inp3,inp4 = next(iter(dataloader_train))
inp1.shape,

history = run.launch(30)

def enable_dropout(network):
    """Function to enable the dropout layers during test-time"""
    network.dropout_layer.train()

n_input_channels = 6
n_assets = 10
max_weight = 0.2
hidden_size = 32
network2 = BachelierNet(n_input_channels, n_assets, hidden_size=hidden_size, max_weight=max_weight)

print(network2)

enable_dropout(network2)

run2 = Run(network2,
          loss,
          dataloader_train,
          val_dataloaders={'test': dataloader_test},
          optimizer=torch.optim.Adam(network2.parameters(), amsgrad=True),
          callbacks=[EarlyStoppingCallback(metric_name='loss',
                                           dataloader_name='test',
                                           patience=15)])

history = run2.launch(30)

network=network.eval()
network2=network2.eval()

benchmarks = {
    '1overN': OneOverN(),  # each asset has weight 1 / n_assets
    'random': Random(),  # random allocation that is however close 1OverN
    'network': network,
    'Monte Carlo': network2
}

metrics = {
    'MaxDD': WorstReturn(),
    'MeanReturn': MeanReturns(),
    'Sharpe': SharpeRatio(rf=1e-4, returns_channel=0, input_type='simple', output_type='simple', eps=1e-4)
    
}

type(dataloader_test)

metrics_table = generate_metrics_table(benchmarks,
                                       dataloader_test,
                                       metrics)

metrics_table

plot_metrics(metrics_table)

dict=generate_cumrets(benchmarks,dataloader_test)

dict["1overN"]

"""**Variational RNN**"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import deepdow
from deepdow import utils
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from blitz.modules import BayesianLSTM, BayesianLinear
from blitz.utils import variational_estimator

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
# %matplotlib inline

from collections import deque

window_size = 21

def create_timestamps_ds(series, 
                         timestep_size=window_size):
    time_stamps = []
    labels = []
    aux_deque = deque(maxlen=timestep_size)
    
    #starting the timestep deque
    for i in range(timestep_size):
        aux_deque.append(0)
    
    #feed the timestamps list
    for i in range(len(series)-1):
        aux_deque.append(series[i])
        time_stamps.append(list(aux_deque))
    
    #feed the labels lsit
    for i in range(len(series)-1):
        labels.append(series[i + 1])
    
    assert len(time_stamps) == len(labels), "Something went wrong"
    
    #torch-tensoring it
    features = torch.tensor(time_stamps[timestep_size:]).float()
    labels = torch.tensor(labels[timestep_size:]).float()
    
    return features, labels

from deepdow.utils import prices_to_returns, returns_to_Xy

def raw_to_Xy(
    raw_data,
    lookback=21,
    horizon=1,
    gap=1,
    freq="B",
    included_assets=None,
    included_indicators=None,
    use_log=True,
):
    """Convert raw data to features.

    Parameters
    ----------
    raw_data : pd.DataFrame
        Rows represents different timestamps stored in index. Note that there can be gaps. Columns are pd.MultiIndex
        with the zero level being assets and the first level indicator.

    lookback : int
        Number of timesteps to include in the features.

    horizon : int
        Number of timesteps to included in the label.

    gap : int
        Integer representing the number of time periods one cannot act after observing the features.

    freq : str
        Periodicity of the data.

    included_assets : None or list
        Assets to be included. If None then all available.

    included_indicators : None or list
        Indicators to be included. If None then all available.

    use_log : bool
        If True, then logarithmic returns are used (natural logarithm). If False, then simple returns.

    Returns
    -------
    X : np.ndarray
        Feature array of shape `(n_samples, n_indicators, lookback, n_assets)`.

    timestamps : pd.DateTimeIndex
        Per row timestamp of shape length `n_samples`.

    y : np.ndarray
        Targets array of shape `(n_samples, n_indicators, horizon, n_assets)`.

    asset_names : list
        Names of assets.

    indicators : list
        List of indicators.
    """
    if freq is None:
        raise ValueError("Frequency freq needs to be specified.")

    asset_names = (
        included_assets
        if included_assets is not None
        else raw_data.columns.levels[0].to_list()
    )
    indicators = (
        included_indicators
        if included_indicators is not None
        else raw_data.columns.levels[1].to_list()
    )

    index = pd.date_range(start=raw_data.index[0], end=raw_data.index[-1], freq=freq)

    new = pd.DataFrame(raw_data, index=index).ffill().bfill()

    to_exclude = []
    for a in asset_names:
        is_valid = np.all(np.isfinite(new[a])) and np.all(new[a] > 0)
        if not is_valid:
            print(a, "is not valid")
            to_exclude.append(a)

    asset_names = sorted(list(set(asset_names) - set(to_exclude)))

    absolute = new.iloc[:, new.columns.get_level_values(0).isin(asset_names)][
        asset_names
    ]  # sort
    absolute = absolute.iloc[:, absolute.columns.get_level_values(1).isin(indicators)]

    returns = prices_to_returns(absolute, use_log=use_log)

    X_list = []
    y_list = []
    for ind in indicators:
        X, timestamps, y = returns_to_Xy(
            returns.xs(ind, axis=1, level=1),
            lookback=lookback,
            horizon=horizon,
            gap=gap,
        )
        X_list.append(X)
        y_list.append(y)

    X = np.concatenate(X_list, axis=1)
    y = np.concatenate(y_list, axis=1)

    return X, timestamps, y, asset_names, indicators

X, timestamps, y, asset_names, indicators = raw_to_Xy(raw_df,
                                                     lookback=window_size,
                                                      gap=1,
                                                      freq="B",
                                                      horizon=1, included_indicators=["Close"])

class VariationalRNN(nn.Module):
    """Recurrent neural network layer.

    Parameters
    ----------
    n_channels : int
        Number of input channels.

    hidden_size : int
        Hidden state size. Alternatively one can see it as number of output channels.

    cell_type : str, {'LSTM', 'RNN'}
        Type of the recurrent cell.

    bidirectional : bool
        If True, then bidirectional. Note that `hidden_size` already takes this parameter into account.

    n_layers : int
        Number of stacked layers.

    """

    def __init__(self, n_channels, hidden_size, cell_type='LSTM', bidirectional=True, n_layers=1):
        """Construct."""
        super().__init__()

        if hidden_size % 2 != 0 and bidirectional:
            raise ValueError('Hidden size needs to be divisible by two for bidirectional RNNs.')

        hidden_size_one_direction = int(hidden_size // (1 + int(bidirectional)))  # only will work out for

        if cell_type == 'RNN':
            self.cell = torch.nn.RNN(n_channels, hidden_size_one_direction, bidirectional=bidirectional,
                                     num_layers=n_layers)

        elif cell_type == 'LSTM':
            self.cell = BayesianLSTM(n_channels, hidden_size)

        else:
            raise ValueError('Unsupported cell_type {}'.format(cell_type))

    def forward(self, x):
        """Perform forward pass.

        Parameters
        ----------
        x : torch.Tensor
            Tensor of shape `(n_samples, n_channels, lookback, n_assets)`.

        Returns
        -------
        torch.Tensor
            Tensor of shape `(n_samples, self.hidden_size, lookback, n_assets)`.

        """
        n_samples, n_channels, lookback, n_assets = x.shape
        x_swapped = x.permute(0, 2, 3, 1)  # n_samples, lookback, n_assets, n_channels
        res = []

        for i in range(n_samples):
            all_hidden_ = self.cell(x_swapped[i])[0]  # lookback, n_assets, hidden_size
            res.append(all_hidden_.permute(2, 0, 1))  # hidden_size, lookback, n_assets

        return torch.stack(res)

@variational_estimator
class NN(nn.Module):
    def __init__(self,n_assets, hidden_size=32, max_weight=0.3, shrinkage_strategy='diagonal',lookback=21):
        super(NN, self).__init__()
        self.transform_layer = VariationalRNN(1, hidden_size=hidden_size)
        self.time_collapse_layer = AttentionCollapse(n_channels=hidden_size)
        self.covariance_layer = CovarianceMatrix(sqrt=False, shrinkage_strategy=shrinkage_strategy)
        self.channel_collapse_layer = AverageCollapse(collapse_dim=1)
        self.portfolio_opt_layer = NumericalMarkowitz(n_assets, max_weight=max_weight)
        self.gamma_sqrt = torch.nn.Parameter(torch.ones(1), requires_grad=True)
        self.alpha = torch.nn.Parameter(torch.ones(1), requires_grad=True)

    def forward(self, x):
        x = x.unsqueeze(1)
        # Covmat
        rets = x[:, 0, :, :]
        covmat = self.covariance_layer(rets)

        # expected returns
        x = self.transform_layer(x)
        x = self.time_collapse_layer(x)
        exp_rets = self.channel_collapse_layer(x)

        # gamma
        gamma_sqrt_all = torch.ones(len(x)).to(device=x.device, dtype=x.dtype) * self.gamma_sqrt
        alpha_all = torch.ones(len(x)).to(device=x.device, dtype=x.dtype) * self.alpha

        # weights
        weights = self.portfolio_opt_layer(exp_rets, covmat, gamma_sqrt_all, alpha_all)

        return weights

criterion=SortinoRatio()

#Xs, ys = create_timestamps_ds(data_returns)
from tkinter import Y


X = torch.Tensor(X).squeeze()
y = torch.Tensor(y).squeeze()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=.2,
                                                    random_state=42,
                                                    shuffle=False)

y_test=y_test.unsqueeze(1)
y_test=y_test.unsqueeze(1)

ds = torch.utils.data.TensorDataset(X_train, y_train)
dataloader_train = torch.utils.data.DataLoader(ds, batch_size=8, shuffle=True)

net = NN(n_assets=10)

optimizer = optim.Adam(net.parameters(), lr=0.001)

y_test

iteration = 0
for epoch in range(50):
    for i, (datapoints, labels) in enumerate(dataloader_train):
        optimizer.zero_grad()
    
        labels=labels.unsqueeze(1)
        labels=labels.unsqueeze(1)
        loss = net.sample_elbo(inputs=datapoints,
                               labels=labels,
                               criterion=criterion,
                               sample_nbr=3,
                               complexity_cost_weight=1/X_train.shape[0])
        loss=loss.mean()
        loss.backward()
        optimizer.step()
        
        iteration += 1
        if iteration%100==0:
            preds_test = net(X_test)[:]
            print (preds_test.shape)

            print (y_test.shape)

            loss_test = criterion(preds_test, y_test)
            loss_test=  loss_test.mean()
            print("Iteration: {} Val-loss: {:.4f}".format(str(iteration), loss_test))

daily_ret=preds_test*y_test.squeeze()

daily_ret

daily_sum=daily_ret.sum(1)

y_test=y_test.squeeze(1)

from deepdow.losses import portfolio_cumulative_returns
cumulative_sum_bayesian=portfolio_cumulative_returns(weights=preds_test, y=y_test, input_type='log', output_type='simple', rebalance=False)
cumulative_sum_bayesian

len(cumulative_sum_bayesian)

dict.keys()

dict["network"]
concatted=pd.concat([dict["1overN"],dict["random"],dict["network"]],axis=1)
concatted.reset_index(drop=True,inplace=True)
concatted.columns=['1overN', 'random', 'network']
concatted

np_cum=cumulative_sum_bayesian.cpu().detach().numpy()

deleted=np.delete(np_cum,range(23))

concatted["bayesian"]=deleted
concatted

concatted.plot()

y_test=y_test.squeeze(1)

y_test.shape

len(daily_sum)

table=pd.DataFrame(daily_sum.cpu().detach().numpy())

table

table_cumulative=table.cumsum()
table_cumulative

dict.keys()

#concatted=

"""**Results**"""

prices_to_returns(data).to_numpy()*preds_test

np_arr = preds_test.cpu().detach().numpy()

np_arr[0]

np_data=prices_to_returns(data[test])

per_epoch_results = history.metrics.groupby(['dataloader', 'metric', 'model', 'epoch'])['value']

per_epoch_results.mean()['test']['loss']['network'].plot()

network = network.eval()

benchmarks = {
    '1overN': OneOverN(),  # each asset has weight 1 / n_assets
    'random': Random(),  # random allocation that is however close 1OverN
    'network': network
}

metrics = {
    'MaxDD': MaximumDrawdown(),
    'Sharpe': SharpeRatio(),
    'MeanReturn': MeanReturns()
}

dataloader_test.squeeze()

metrics_table = generate_metrics_table(benchmarks,
                                       dataloader_test,
                                       metrics)

metrics_table.to_csv("metrics_table2.csv")

metrics_table["value"]

plot_metrics(metrics_table)

def plot_metrics(metrics_table):
    """Plot performance of all benchmarks for all metrics.

    Parameters
    ----------
    metrics_table : pd.DataFrame
        Table with the following columns - 'metric', 'timestamp', 'benchmark' and 'value'.

    Returns
    -------
    return_ax : 'matplotlib.axes._subplots.AxesSubplot
        Axes with number of subaxes equal to number of metrics.

    """
    all_metrics = metrics_table['metric'].unique()
    n_metrics = len(all_metrics)

    _, axs = plt.subplots(n_metrics)

    for i, metric_name in enumerate(all_metrics):
        df = pd.pivot_table(metrics_table[metrics_table['metric'] == metric_name],
                            values='value',
                            columns='benchmark',
                            index='timestamp').sort_index()
        df.plot(ax=axs[i])
        axs[i].set_title(metric_name)

    plt.tight_layout()

    return axs

all_metrics = metrics_table[metrics_table["metric"]!="Sharpe"]['metric'].unique()
n_metrics = len(all_metrics)

_, axs = plt.subplots(n_metrics)
for i, metric_name in enumerate(all_metrics):
    df = pd.pivot_table(metrics_table[metrics_table['metric'] == metric_name],
                            values='value',
                            columns='benchmark',
                            index='timestamp').sort_index()
    df.plot(ax=axs[i])
    axs[i].set_title(metric_name)
    plt.tight_layout()

axs



tensor = next(iter(dataloader_test))[0]

tensor_temp = []


tensor_temp[0].shape

tensor = next(iter(dataloader_test))[0]
tensor.shape

output = network(next(iter(dataloader_test))[0].to(torch.float32))
output[0]

features, returns, idx, names = dataset[593]
features

raw_df.iloc[613]

enable_dropout(network)
last_allocation = network(dataset[614][0].to(torch.float32).unsqueeze(0))
last_allocation

last_allocation.sum()

per_epoch_results = history.metrics.groupby(['dataloader', 'metric', 'model', 'epoch'])['value']

network = network.eval()

benchmarks = {
    '1overN': OneOverN(),  # each asset has weight 1 / n_assets
    'random': Random(),  # random allocation that is however close 1OverN
    'network': network
}

metrics = {
    'MaxDD': MaximumDrawdown(),
    'Sharpe': SharpeRatio(),
    'MeanReturn': MeanReturns()
}

metrics_table = generate_metrics_table(benchmarks,
                                       dataloader_test,
                                       metrics)

plot_metrics(metrics_table)

weight_table = generate_weights_table(network, dataloader_test)



plot_weight_heatmap(weight_table,
                    add_sum_column=True,
                    time_format=None,
                    time_skips=25)

import talos

p = {'activation':['relu', 'elu'],
       'optimizer': ['Nadam', 'Adam'],
       'hidden_layers':[0, 1, 2],
       'batch_size': (20, 50, 5),
       'epochs': [10, 20]}

scan_object = talos.Scan(x=dataloader_train.dataset.X,
                         y=dataloader_train.dataset.y,
                         x_val=dataloader_test.dataset.X,
                         y_val=dataloader_test.dataset.y,
                         params=p,
                         model=network,
                         experiment_name='returns',
                         round_limit=100)